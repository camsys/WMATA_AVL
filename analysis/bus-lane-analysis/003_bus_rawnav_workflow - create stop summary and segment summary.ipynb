{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find nearest stops and segment ends "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-25T16:53:39.139133Z",
     "start_time": "2020-09-25T16:53:38.339394Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Section 1 Import Libraries and Set Global Parameters...\n"
     ]
    }
   ],
   "source": [
    "# 0 Housekeeping. Clear variable space\n",
    "########################################################################################################################\n",
    "from IPython import get_ipython  # run magic commands\n",
    "ipython = get_ipython()\n",
    "ipython.magic(\"reset -f\")\n",
    "ipython = get_ipython()\n",
    "#https://stackoverflow.com/questions/36572282/ipython-autoreload-magic-function-not-found\n",
    "ipython.magic(\"load_ext autoreload\")\n",
    "ipython.magic(\"autoreload 2\")\n",
    "# 1 Import Libraries and Set Global Parameters\n",
    "########################################################################################################################\n",
    "# 1.1 Import Python Libraries\n",
    "############################################\n",
    "from datetime import datetime\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import shutil\n",
    "print(\"Run Section 1 Import Libraries and Set Global Parameters...\")\n",
    "begin_time = datetime.now()\n",
    "import os, sys, pandas as pd, geopandas as gpd\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")  # Stop Pandas warnings\n",
    "    \n",
    "path_working = r\"C:\\Users\\E048374\\OneDrive - WMATA\\rawnav_rachel_fork\\WMATA_AVL\"\n",
    "os.chdir(os.path.join(path_working))\n",
    "sys.path.append(r\"C:\\Users\\E048374\\OneDrive - WMATA\\rawnav_rachel_fork\\WMATA_AVL\")\n",
    "path_source_data = r\"\\\\l-600730\\RawNavArchive\"\n",
    "path_sp = r\"C:\\Users\\E048374\\Documents\\RawNav\"\n",
    "path_processed_data = os.path.join(path_working, \"data\", \"02-processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-25T16:53:39.474610Z",
     "start_time": "2020-09-25T16:53:39.396613Z"
    }
   },
   "outputs": [],
   "source": [
    "# Globals\n",
    "\n",
    "q_jump_route_list = ['52']\n",
    "pattern_id = '5201' # see schedule filename below -- probably could just have full schedule..\n",
    "analysis_routes = q_jump_route_list\n",
    "analysis_days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "wmata_crs = 2248"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-25T16:53:41.577510Z",
     "start_time": "2020-09-25T16:53:40.329119Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Time Section 1 Import Libraries and Set Global Parameters : 0:00:02\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# 1.3 Import User-Defined Package\n",
    "############################################\n",
    "import wmatarawnav as wr\n",
    "\n",
    "executionTime = str(datetime.now() - begin_time).split('.')[0]\n",
    "print(\"Run Time Section 1 Import Libraries and Set Global Parameters : {}\".format(executionTime))\n",
    "print(\"*\" * 100)\n",
    "\n",
    "wmata_schedule_dat = (\n",
    "    pd.read_csv(\n",
    "        os.path.join(path_processed_data, f\"bus_sched_{pattern_id}.csv\")\n",
    "        ,dtype={'pattern':'int32','route':'str'}\n",
    "    )\n",
    ")\n",
    "\n",
    "wmata_schedule_gdf = (\n",
    "    gpd.GeoDataFrame(\n",
    "        wmata_schedule_dat, \n",
    "        geometry = gpd.points_from_xy(wmata_schedule_dat.stop_lon,wmata_schedule_dat.stop_lat),\n",
    "        crs='EPSG:4326'\n",
    "    )\n",
    "    .to_crs(epsg=wmata_crs)\n",
    ")\n",
    "\n",
    "# Make Output Directory\n",
    "path_stop_summary = os.path.join(path_processed_data, \"stop_summary.parquet\")\n",
    "if not os.path.isdir(path_stop_summary):\n",
    "    os.mkdir(path_stop_summary)\n",
    "\n",
    "path_stop_index = os.path.join(path_processed_data, \"stop_index.parquet\")\n",
    "if not os.path.isdir(path_stop_index):\n",
    "    os.mkdir(path_stop_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-25T16:55:58.952427Z",
     "start_time": "2020-09-25T16:53:43.482763Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "Processing analysis route 52\n",
      "Processing analysis route 52 for Monday...\n",
      "deleted 188 rows of 2960 rows with distance to the nearest stop > 100 ft. from index table\n",
      "deleted 10 of 2772 stops with incorrect order from index table\n",
      "Processing analysis route 52 for Tuesday...\n",
      "deleted 9 rows of 2640 rows with distance to the nearest stop > 100 ft. from index table\n",
      "deleted 0 of 2631 stops with incorrect order from index table\n",
      "Processing analysis route 52 for Wednesday...\n",
      "deleted 75 rows of 2520 rows with distance to the nearest stop > 100 ft. from index table\n",
      "deleted 1 of 2445 stops with incorrect order from index table\n",
      "Processing analysis route 52 for Thursday...\n",
      "deleted 145 rows of 3280 rows with distance to the nearest stop > 100 ft. from index table\n",
      "deleted 6 of 3135 stops with incorrect order from index table\n",
      "Processing analysis route 52 for Friday...\n",
      "deleted 87 rows of 3080 rows with distance to the nearest stop > 100 ft. from index table\n",
      "deleted 0 of 2993 stops with incorrect order from index table\n",
      "Processing analysis route 52 for Saturday...\n",
      "deleted 36 rows of 3000 rows with distance to the nearest stop > 100 ft. from index table\n",
      "deleted 0 of 2964 stops with incorrect order from index table\n",
      "Processing analysis route 52 for Sunday...\n",
      "deleted 61 rows of 1600 rows with distance to the nearest stop > 100 ft. from index table\n",
      "deleted 0 of 1539 stops with incorrect order from index table\n",
      "Run Time Section Section 2: Read, analyze and summarize rawnav, WMATA schedule data : 0:02:20\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "for analysis_route in analysis_routes:\n",
    "    print(\"*\" * 100)\n",
    "    print('Processing analysis route {}'.format(analysis_route))\n",
    "    for analysis_day in analysis_days:\n",
    "        print('Processing analysis route {} for {}...'.format(analysis_route,analysis_day))\n",
    "                \n",
    "        # Reload data\n",
    "        try:\n",
    "            rawnav_dat = (\n",
    "                wr.read_cleaned_rawnav(\n",
    "                   analysis_routes_ = analysis_route,\n",
    "                   analysis_days_ = analysis_day,\n",
    "                   path = os.path.join(path_processed_data, \"rawnav_data.parquet\")\n",
    "                )\n",
    "                .drop(columns=['blank', 'lat_raw', 'long_raw', 'sat_cnt'])\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(e)  # usually no data found or something similar\n",
    "            continue\n",
    "        else:\n",
    "\n",
    "            rawnav_summary_dat = (\n",
    "                wr.read_cleaned_rawnav(\n",
    "                    analysis_routes_ = analysis_route,\n",
    "                    analysis_days_ = analysis_day,\n",
    "                    path = os.path.join(path_processed_data, \"rawnav_summary.parquet\")\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Subset Rawnav Data to Records Desired\n",
    "            rawnav_summary_dat = rawnav_summary_dat.query('not (run_duration_from_sec < 600 | dist_odom_mi < 2)')\n",
    "            \n",
    "            rawnav_summary_keys_col = rawnav_summary_dat[['filename', 'index_run_start']]\n",
    "            \n",
    "            rawnav_qjump_dat = rawnav_dat.merge(rawnav_summary_keys_col,\n",
    "                                                on=['filename', 'index_run_start'],\n",
    "                                                how='right')\n",
    "\n",
    "            rawnav_qjump_gdf = (\n",
    "                gpd.GeoDataFrame(\n",
    "                    rawnav_qjump_dat,\n",
    "                    geometry=gpd.points_from_xy(rawnav_qjump_dat.long, rawnav_qjump_dat.lat),\n",
    "                    crs='EPSG:4326'\n",
    "                )\n",
    "                .to_crs(epsg=wmata_crs)\n",
    "            )\n",
    "\n",
    "        stop_summary, stop_index = (\n",
    "            wr.merge_rawnav_wmata_schedule(\n",
    "                analysis_route_=analysis_route,\n",
    "                analysis_day_=analysis_day,\n",
    "                rawnav_dat_=rawnav_qjump_gdf,\n",
    "                rawnav_sum_dat_=rawnav_summary_dat,\n",
    "                wmata_schedule_dat_=wmata_schedule_gdf\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        if type(stop_summary) == type(None):\n",
    "            print('No data on analysis route {} for {}'.format(analysis_route,analysis_day))\n",
    "            continue\n",
    "        \n",
    "        # Write Summary Table \n",
    "        shutil.rmtree(\n",
    "            os.path.join(\n",
    "                path_stop_summary,\n",
    "                \"route={}\".format(analysis_route),\n",
    "                \"wday={}\".format(analysis_day)\n",
    "            ),\n",
    "            ignore_errors=True\n",
    "        ) \n",
    "        \n",
    "        pq.write_to_dataset(\n",
    "            table=pa.Table.from_pandas(stop_summary),\n",
    "            root_path=path_stop_summary,\n",
    "            partition_cols=['route', 'wday']\n",
    "        )\n",
    "        \n",
    "        # Write Index Table\n",
    "        shutil.rmtree(\n",
    "            os.path.join(\n",
    "                path_stop_index,\n",
    "                \"route={}\".format(analysis_route),\n",
    "                \"wday={}\".format(analysis_day)\n",
    "            ),\n",
    "            ignore_errors=True\n",
    "        ) \n",
    "        \n",
    "        stop_index = wr.drop_geometry(stop_index)\n",
    "        \n",
    "        stop_index = stop_index.assign(wday=analysis_day)\n",
    "                \n",
    "        pq.write_to_dataset(\n",
    "            table=pa.Table.from_pandas(stop_index),\n",
    "            root_path=path_stop_index,\n",
    "            partition_cols=['route', 'wday']\n",
    "        )\n",
    "\n",
    "executionTime = str(datetime.now() - begin_time).split('.')[0]\n",
    "print(\n",
    "      \"Run Time Section Section 2: Read, analyze and summarize rawnav, WMATA schedule data : {}\"\n",
    "      .format(executionTime)\n",
    ")\n",
    "print(\"*\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find nearest rawnav to segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-25T16:58:48.361406Z",
     "start_time": "2020-09-25T16:58:48.019655Z"
    }
   },
   "outputs": [],
   "source": [
    "segments = (\n",
    "    gpd.read_file(os.path.join(path_processed_data,\"seg_5201_by_intersection.geojson\"), dtype={'pattern':'int32'})\n",
    "    .to_crs(wmata_crs)\n",
    ")[['seg_name_id', 'name_str', 'geoid', 'stop_id',\n",
    "       'length', 'geometry']]\n",
    "\n",
    "seg_pattern = pd.read_csv(os.path.join(path_processed_data,\"stop_seq_pattern_5201_by_intersection.csv\"),\n",
    "                         dtype={'route':str, 'PATTERN_ID':str, 'pattern':'int32'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-25T16:58:48.966456Z",
     "start_time": "2020-09-25T16:58:48.709542Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make Output Directory\n",
    "path_seg_summary = os.path.join(path_processed_data, \"segment_summary.parquet\")\n",
    "shutil.rmtree(path_seg_summary, ignore_errors=True) \n",
    "os.mkdir(path_seg_summary)\n",
    "\n",
    "path_seg_index = os.path.join(path_processed_data, \"segment_index.parquet\")\n",
    "shutil.rmtree(path_seg_index, ignore_errors=True) \n",
    "os.mkdir(path_seg_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-25T17:04:31.966289Z",
     "start_time": "2020-09-25T16:58:49.301977Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "Processing analysis route 52\n",
      "Processing analysis route 52 for Monday...\n",
      "Processing segment 14th_12 ...\n",
      "Processing segment 14th_14 ...\n",
      "Processing segment 14th_17 ...\n",
      "Processing segment 14th_21 ...\n",
      "Processing segment 14th_22 ...\n",
      "Processing segment 14th_24 ...\n",
      "Processing segment 14th_25 ...\n",
      "Processing analysis route 52 for Tuesday...\n",
      "Processing segment 14th_12 ...\n",
      "Processing segment 14th_14 ...\n",
      "Processing segment 14th_17 ...\n",
      "Processing segment 14th_21 ...\n",
      "Processing segment 14th_22 ...\n",
      "Processing segment 14th_24 ...\n",
      "Processing segment 14th_25 ...\n",
      "Processing analysis route 52 for Wednesday...\n",
      "Processing segment 14th_12 ...\n",
      "Processing segment 14th_14 ...\n",
      "Processing segment 14th_17 ...\n",
      "Processing segment 14th_21 ...\n",
      "Processing segment 14th_22 ...\n",
      "Processing segment 14th_24 ...\n",
      "Processing segment 14th_25 ...\n",
      "Processing analysis route 52 for Thursday...\n",
      "Processing segment 14th_12 ...\n",
      "Processing segment 14th_14 ...\n",
      "Processing segment 14th_17 ...\n",
      "Processing segment 14th_21 ...\n",
      "Processing segment 14th_22 ...\n",
      "Processing segment 14th_24 ...\n",
      "Processing segment 14th_25 ...\n",
      "Processing analysis route 52 for Friday...\n",
      "Processing segment 14th_12 ...\n",
      "Processing segment 14th_14 ...\n",
      "Processing segment 14th_17 ...\n",
      "Processing segment 14th_21 ...\n",
      "Processing segment 14th_22 ...\n",
      "Processing segment 14th_24 ...\n",
      "Processing segment 14th_25 ...\n",
      "Processing analysis route 52 for Saturday...\n",
      "Processing segment 14th_12 ...\n",
      "Processing segment 14th_14 ...\n",
      "Processing segment 14th_17 ...\n",
      "Processing segment 14th_21 ...\n",
      "Processing segment 14th_22 ...\n",
      "Processing segment 14th_24 ...\n",
      "Processing segment 14th_25 ...\n",
      "Processing analysis route 52 for Sunday...\n",
      "Processing segment 14th_12 ...\n",
      "Processing segment 14th_14 ...\n",
      "Processing segment 14th_17 ...\n",
      "Processing segment 14th_21 ...\n",
      "Processing segment 14th_22 ...\n",
      "Processing segment 14th_24 ...\n",
      "Processing segment 14th_25 ...\n"
     ]
    }
   ],
   "source": [
    "# 3 Merge Additional Geometry\n",
    "####################################################################################################\n",
    "\n",
    "# 3.1 Rawnav-Segment ########################\n",
    "# Iterate\n",
    "for analysis_route in analysis_routes:\n",
    "    print(\"*\" * 100)\n",
    "    print(f'Processing analysis route {analysis_route}')\n",
    "    for analysis_day in analysis_days:\n",
    "        print(f'Processing analysis route {analysis_route} for {analysis_day}...')\n",
    "        \n",
    "        # Reload data\n",
    "        try:\n",
    "            rawnav_dat = (\n",
    "                wr.read_cleaned_rawnav(\n",
    "                   analysis_routes_ = analysis_route,\n",
    "                   analysis_days_ = analysis_day,\n",
    "                   path = os.path.join(path_processed_data, \"rawnav_data.parquet\"))\n",
    "                .drop(columns=['blank', 'lat_raw', 'long_raw', 'sat_cnt'])\n",
    "                )\n",
    "        except:\n",
    "            print(f'No data on analysis route {analysis_route} for {analysis_day}')\n",
    "            continue\n",
    "        else:\n",
    "   \n",
    "            # Reload Data\n",
    "            rawnav_summary_dat = (\n",
    "                wr.read_cleaned_rawnav(\n",
    "                    analysis_routes_ = analysis_route,\n",
    "                    analysis_days_ = analysis_day,\n",
    "                    path = os.path.join(path_processed_data, \"rawnav_summary.parquet\")\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Subset Rawnav Data to Records Desired\n",
    "            rawnav_summary_dat = rawnav_summary_dat.query('not (run_duration_from_sec < 600 | dist_odom_mi < 2)')\n",
    "            \n",
    "            rawnav_qjump_dat = rawnav_dat.merge(rawnav_summary_dat[['filename', 'index_run_start']], \n",
    "                                                on=['filename', 'index_run_start'],\n",
    "                                                how='right')\n",
    "            \n",
    "            # Address Remaining Col Format issues\n",
    "            rawnav_qjump_gdf = (\n",
    "                gpd.GeoDataFrame(\n",
    "                    rawnav_qjump_dat, \n",
    "                    geometry = gpd.points_from_xy(\n",
    "                        rawnav_qjump_dat.long,\n",
    "                        rawnav_qjump_dat.lat\n",
    "                    ),\n",
    "                    crs='EPSG:4326')\n",
    "                .to_crs(epsg=wmata_crs)\n",
    "            )\n",
    "    \n",
    "            # Iterate on over Pattern-Segments Combinations Applicable to Route\n",
    "            xwalk_seg_pattern_subset = seg_pattern[['route','pattern','seg_name_id']].copy()\n",
    "                        \n",
    "            for seg in xwalk_seg_pattern_subset.seg_name_id.unique():\n",
    "                print('Processing segment {} ...'.format(seg))\n",
    "\n",
    "                # We pass the rawnav data and summary tables, check against a segment,\n",
    "                # and use the patterns_by_seg to indicate which patterns should be examined\n",
    "                index_run_segment_start_end, summary_run_segment = (\n",
    "                    wr.merge_rawnav_segment(\n",
    "                        rawnav_gdf_=rawnav_qjump_gdf,\n",
    "                        rawnav_sum_dat_=rawnav_summary_dat,\n",
    "                        target_=segments.loc[segments.seg_name_id == seg],\n",
    "                        patterns_by_seg_=xwalk_seg_pattern_subset.loc[xwalk_seg_pattern_subset.seg_name_id == seg]\n",
    "                    )\n",
    "                )\n",
    "                # Note that because seg_pattern_first_last is defined for route and pattern,\n",
    "                # our summary will implicitly drop any runs that are on 'wrong' pattern(s) for \n",
    "                # a route. \n",
    "                \n",
    "                index_run_segment_start_end['wday'] = analysis_day\n",
    "                summary_run_segment['wday'] = analysis_day\n",
    "                \n",
    "                # The additional partitioning here is excessive, but if fits better in the \n",
    "                # iterative/chunking process above\n",
    "                pq.write_to_dataset(\n",
    "                    table = pa.Table.from_pandas(summary_run_segment),\n",
    "                    root_path = path_seg_summary,\n",
    "                    partition_cols = ['route','wday','seg_name_id']\n",
    "                )\n",
    "                \n",
    "                pq.write_to_dataset(\n",
    "                    table = pa.Table.from_pandas(index_run_segment_start_end),\n",
    "                    root_path = path_seg_index,\n",
    "                    partition_cols = ['route','wday','seg_name_id']\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rawnav)",
   "language": "python",
   "name": "rawnav"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
